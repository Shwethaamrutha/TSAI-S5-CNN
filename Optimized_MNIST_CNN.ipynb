{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimized MNIST CNN - Target: 99.4% accuracy with <20k parameters in <20 epochs\n",
        "# Required: Batch Normalization, Dropout, Global Average Pooling\n",
        "\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Install required packages\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "\n",
        "# Device configuration\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Model Definitions - All 5 Models\n",
        "\n",
        "# Model 1: Original OptimizedNet (Baseline)\n",
        "class OptimizedNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Original optimized architecture\n",
        "    - Minimal channels for parameter efficiency\n",
        "    - Strategic dropout placement\n",
        "    - Global Average Pooling for parameter reduction\n",
        "    \"\"\"\n",
        "    def __init__(self, dropout_rate=0.1):\n",
        "        super(OptimizedNet, self).__init__()\n",
        "        \n",
        "        # Block 1: Initial feature extraction (minimal channels)\n",
        "        self.conv1 = nn.Conv2d(1, 8, 3, padding=1)  # 28x28 -> 28x28\n",
        "        self.bn1 = nn.BatchNorm2d(8)\n",
        "        self.dropout1 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Block 2: Feature expansion (still minimal)\n",
        "        self.conv2 = nn.Conv2d(8, 16, 3, padding=1)  # 28x28 -> 28x28\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.dropout2 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Transition layer: 1x1 conv for dimensionality reduction\n",
        "        self.transition1 = nn.Conv2d(16, 8, 1)  # 28x28 -> 28x28\n",
        "        self.bn_trans1 = nn.BatchNorm2d(8)\n",
        "        \n",
        "        # MaxPool after transition\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)  # 28x28 -> 14x14\n",
        "        \n",
        "        # Block 3: Mid-level features (moderate channels)\n",
        "        self.conv3 = nn.Conv2d(8, 16, 3, padding=1)  # 14x14 -> 14x14\n",
        "        self.bn3 = nn.BatchNorm2d(16)\n",
        "        self.dropout3 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Block 4: Feature expansion (moderate channels)\n",
        "        self.conv4 = nn.Conv2d(16, 32, 3, padding=1)  # 14x14 -> 14x14\n",
        "        self.bn4 = nn.BatchNorm2d(32)\n",
        "        self.dropout4 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Transition layer: 1x1 conv for dimensionality reduction\n",
        "        self.transition2 = nn.Conv2d(32, 16, 1)  # 14x14 -> 14x14\n",
        "        self.bn_trans2 = nn.BatchNorm2d(16)\n",
        "        \n",
        "        # MaxPool after transition\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)  # 14x14 -> 7x7\n",
        "        \n",
        "        # Block 5: High-level features (moderate channels)\n",
        "        self.conv5 = nn.Conv2d(16, 32, 3, padding=1)  # 7x7 -> 7x7\n",
        "        self.bn5 = nn.BatchNorm2d(32)\n",
        "        self.dropout5 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Block 6: Final feature extraction (reduce for GAP)\n",
        "        self.conv6 = nn.Conv2d(32, 16, 3, padding=1)  # 7x7 -> 7x7\n",
        "        self.bn6 = nn.BatchNorm2d(16)\n",
        "        self.dropout6 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Global Average Pooling\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)  # 7x7 -> 1x1\n",
        "        \n",
        "        # Final classifier\n",
        "        self.fc = nn.Linear(16, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Block 1\n",
        "        x = self.dropout1(F.relu(self.bn1(self.conv1(x))))\n",
        "        \n",
        "        # Block 2\n",
        "        x = self.dropout2(F.relu(self.bn2(self.conv2(x))))\n",
        "        \n",
        "        # Transition 1\n",
        "        x = F.relu(self.bn_trans1(self.transition1(x)))\n",
        "        x = self.pool1(x)\n",
        "        \n",
        "        # Block 3\n",
        "        x = self.dropout3(F.relu(self.bn3(self.conv3(x))))\n",
        "        \n",
        "        # Block 4\n",
        "        x = self.dropout4(F.relu(self.bn4(self.conv4(x))))\n",
        "        \n",
        "        # Transition 2\n",
        "        x = F.relu(self.bn_trans2(self.transition2(x)))\n",
        "        x = self.pool2(x)\n",
        "        \n",
        "        # Block 5\n",
        "        x = self.dropout5(F.relu(self.bn5(self.conv5(x))))\n",
        "        \n",
        "        # Block 6\n",
        "        x = self.dropout6(F.relu(self.bn6(self.conv6(x))))\n",
        "        \n",
        "        # Global Average Pooling\n",
        "        x = self.gap(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # Final classification\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "# Model 2: Improved Architecture (Winner)\n",
        "class OptimizedNetV2(nn.Module):\n",
        "    \"\"\"\n",
        "    Improved architecture with better channel progression\n",
        "    - Better initial learning dynamics\n",
        "    - Controlled capacity for parameter constraint\n",
        "    - Optimized dropout rates\n",
        "    \"\"\"\n",
        "    def __init__(self, dropout_rate=0.05):\n",
        "        super(OptimizedNetV2, self).__init__()\n",
        "        \n",
        "        # Block 1: Better initial feature extraction (reduced channels)\n",
        "        self.conv1 = nn.Conv2d(1, 10, 3, padding=1)  # 28x28 -> 28x28\n",
        "        self.bn1 = nn.BatchNorm2d(10)\n",
        "        self.dropout1 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Block 2: Feature expansion with controlled capacity\n",
        "        self.conv2 = nn.Conv2d(10, 16, 3, padding=1)  # 28x28 -> 28x28\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.dropout2 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Transition layer: 1x1 conv for dimensionality reduction\n",
        "        self.transition1 = nn.Conv2d(16, 10, 1)  # 28x28 -> 28x28\n",
        "        self.bn_trans1 = nn.BatchNorm2d(10)\n",
        "        \n",
        "        # MaxPool after transition\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)  # 28x28 -> 14x14\n",
        "        \n",
        "        # Block 3: Mid-level features with controlled capacity\n",
        "        self.conv3 = nn.Conv2d(10, 20, 3, padding=1)  # 14x14 -> 14x14\n",
        "        self.bn3 = nn.BatchNorm2d(20)\n",
        "        self.dropout3 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Block 4: Feature expansion\n",
        "        self.conv4 = nn.Conv2d(20, 28, 3, padding=1)  # 14x14 -> 14x14\n",
        "        self.bn4 = nn.BatchNorm2d(28)\n",
        "        self.dropout4 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Transition layer: 1x1 conv for dimensionality reduction\n",
        "        self.transition2 = nn.Conv2d(28, 16, 1)  # 14x14 -> 14x14\n",
        "        self.bn_trans2 = nn.BatchNorm2d(16)\n",
        "        \n",
        "        # MaxPool after transition\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)  # 14x14 -> 7x7\n",
        "        \n",
        "        # Block 5: High-level features\n",
        "        self.conv5 = nn.Conv2d(16, 24, 3, padding=1)  # 7x7 -> 7x7\n",
        "        self.bn5 = nn.BatchNorm2d(24)\n",
        "        self.dropout5 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Block 6: Final feature extraction\n",
        "        self.conv6 = nn.Conv2d(24, 16, 3, padding=1)  # 7x7 -> 7x7\n",
        "        self.bn6 = nn.BatchNorm2d(16)\n",
        "        self.dropout6 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Global Average Pooling\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)  # 7x7 -> 1x1\n",
        "        \n",
        "        # Final classifier\n",
        "        self.fc = nn.Linear(16, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Block 1\n",
        "        x = self.dropout1(F.relu(self.bn1(self.conv1(x))))\n",
        "        \n",
        "        # Block 2\n",
        "        x = self.dropout2(F.relu(self.bn2(self.conv2(x))))\n",
        "        \n",
        "        # Transition 1\n",
        "        x = F.relu(self.bn_trans1(self.transition1(x)))\n",
        "        x = self.pool1(x)\n",
        "        \n",
        "        # Block 3\n",
        "        x = self.dropout3(F.relu(self.bn3(self.conv3(x))))\n",
        "        \n",
        "        # Block 4\n",
        "        x = self.dropout4(F.relu(self.bn4(self.conv4(x))))\n",
        "        \n",
        "        # Transition 2\n",
        "        x = F.relu(self.bn_trans2(self.transition2(x)))\n",
        "        x = self.pool2(x)\n",
        "        \n",
        "        # Block 5\n",
        "        x = self.dropout5(F.relu(self.bn5(self.conv5(x))))\n",
        "        \n",
        "        # Block 6\n",
        "        x = self.dropout6(F.relu(self.bn6(self.conv6(x))))\n",
        "        \n",
        "        # Global Average Pooling\n",
        "        x = self.gap(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # Final classification\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "# Model 3: Balanced Architecture with SGD Optimization\n",
        "class OptimizedNetV3(nn.Module):\n",
        "    \"\"\"\n",
        "    Balanced architecture optimized for SGD training\n",
        "    - Moderate dropout rates (0.01-0.1)\n",
        "    - Better learning dynamics\n",
        "    - Optimized for SGD optimizer\n",
        "    \"\"\"\n",
        "    def __init__(self, dropout_rate=0.02):\n",
        "        super(OptimizedNetV3, self).__init__()\n",
        "        \n",
        "        # Block 1: Initial feature extraction\n",
        "        self.conv1 = nn.Conv2d(1, 10, 3, padding=1)  # 28x28 -> 28x28\n",
        "        self.bn1 = nn.BatchNorm2d(10)\n",
        "        self.dropout1 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Block 2: Feature expansion\n",
        "        self.conv2 = nn.Conv2d(10, 16, 3, padding=1)  # 28x28 -> 28x28\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.dropout2 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Transition layer: 1x1 conv for dimensionality reduction\n",
        "        self.transition1 = nn.Conv2d(16, 10, 1)  # 28x28 -> 28x28\n",
        "        self.bn_trans1 = nn.BatchNorm2d(10)\n",
        "        \n",
        "        # MaxPool after transition\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)  # 28x28 -> 14x14\n",
        "        \n",
        "        # Block 3: Mid-level features\n",
        "        self.conv3 = nn.Conv2d(10, 20, 3, padding=1)  # 14x14 -> 14x14\n",
        "        self.bn3 = nn.BatchNorm2d(20)\n",
        "        self.dropout3 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Block 4: Feature expansion\n",
        "        self.conv4 = nn.Conv2d(20, 32, 3, padding=1)  # 14x14 -> 14x14\n",
        "        self.bn4 = nn.BatchNorm2d(32)\n",
        "        self.dropout4 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Transition layer: 1x1 conv for dimensionality reduction\n",
        "        self.transition2 = nn.Conv2d(32, 16, 1)  # 14x14 -> 14x14\n",
        "        self.bn_trans2 = nn.BatchNorm2d(16)\n",
        "        \n",
        "        # MaxPool after transition\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)  # 14x14 -> 7x7\n",
        "        \n",
        "        # Block 5: High-level features\n",
        "        self.conv5 = nn.Conv2d(16, 24, 3, padding=1)  # 7x7 -> 7x7\n",
        "        self.bn5 = nn.BatchNorm2d(24)\n",
        "        self.dropout5 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Block 6: Final feature extraction\n",
        "        self.conv6 = nn.Conv2d(24, 16, 3, padding=1)  # 7x7 -> 7x7\n",
        "        self.bn6 = nn.BatchNorm2d(16)\n",
        "        self.dropout6 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Global Average Pooling\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)  # 7x7 -> 1x1\n",
        "        \n",
        "        # Final classifier\n",
        "        self.fc = nn.Linear(16, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Block 1\n",
        "        x = self.dropout1(F.relu(self.bn1(self.conv1(x))))\n",
        "        \n",
        "        # Block 2\n",
        "        x = self.dropout2(F.relu(self.bn2(self.conv2(x))))\n",
        "        \n",
        "        # Transition 1\n",
        "        x = F.relu(self.bn_trans1(self.transition1(x)))\n",
        "        x = self.pool1(x)\n",
        "        \n",
        "        # Block 3\n",
        "        x = self.dropout3(F.relu(self.bn3(self.conv3(x))))\n",
        "        \n",
        "        # Block 4\n",
        "        x = self.dropout4(F.relu(self.bn4(self.conv4(x))))\n",
        "        \n",
        "        # Transition 2\n",
        "        x = F.relu(self.bn_trans2(self.transition2(x)))\n",
        "        x = self.pool2(x)\n",
        "        \n",
        "        # Block 5\n",
        "        x = self.dropout5(F.relu(self.bn5(self.conv5(x))))\n",
        "        \n",
        "        # Block 6\n",
        "        x = self.dropout6(F.relu(self.bn6(self.conv6(x))))\n",
        "        \n",
        "        # Global Average Pooling\n",
        "        x = self.gap(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # Final classification\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "# Model 4: Model1 Architecture with SGD Optimization\n",
        "class OptimizedNetV4(nn.Module):\n",
        "    \"\"\"\n",
        "    Model1 architecture optimized for SGD training\n",
        "    - Same architecture as Model1 (8â†’16â†’32â†’16)\n",
        "    - Minimal dropout (0.02) for SGD compatibility\n",
        "    - Optimized for SGD with momentum\n",
        "    \"\"\"\n",
        "    def __init__(self, dropout_rate=0.02):\n",
        "        super(OptimizedNetV4, self).__init__()\n",
        "        \n",
        "        # Block 1: Initial feature extraction (minimal channels)\n",
        "        self.conv1 = nn.Conv2d(1, 8, 3, padding=1)  # 28x28 -> 28x28\n",
        "        self.bn1 = nn.BatchNorm2d(8)\n",
        "        self.dropout1 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Block 2: Feature expansion (still minimal)\n",
        "        self.conv2 = nn.Conv2d(8, 16, 3, padding=1)  # 28x28 -> 28x28\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.dropout2 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Transition layer: 1x1 conv for dimensionality reduction\n",
        "        self.transition1 = nn.Conv2d(16, 8, 1)  # 28x28 -> 28x28\n",
        "        self.bn_trans1 = nn.BatchNorm2d(8)\n",
        "        \n",
        "        # MaxPool after transition\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)  # 28x28 -> 14x14\n",
        "        \n",
        "        # Block 3: Mid-level features (moderate channels)\n",
        "        self.conv3 = nn.Conv2d(8, 16, 3, padding=1)  # 14x14 -> 14x14\n",
        "        self.bn3 = nn.BatchNorm2d(16)\n",
        "        self.dropout3 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Block 4: Feature expansion (moderate channels)\n",
        "        self.conv4 = nn.Conv2d(16, 32, 3, padding=1)  # 14x14 -> 14x14\n",
        "        self.bn4 = nn.BatchNorm2d(32)\n",
        "        self.dropout4 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Transition layer: 1x1 conv for dimensionality reduction\n",
        "        self.transition2 = nn.Conv2d(32, 16, 1)  # 14x14 -> 14x14\n",
        "        self.bn_trans2 = nn.BatchNorm2d(16)\n",
        "        \n",
        "        # MaxPool after transition\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)  # 14x14 -> 7x7\n",
        "        \n",
        "        # Block 5: High-level features (moderate channels)\n",
        "        self.conv5 = nn.Conv2d(16, 32, 3, padding=1)  # 7x7 -> 7x7\n",
        "        self.bn5 = nn.BatchNorm2d(32)\n",
        "        self.dropout5 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Block 6: Final feature extraction (reduce for GAP)\n",
        "        self.conv6 = nn.Conv2d(32, 16, 3, padding=1)  # 7x7 -> 7x7\n",
        "        self.bn6 = nn.BatchNorm2d(16)\n",
        "        self.dropout6 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Global Average Pooling\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)  # 7x7 -> 1x1\n",
        "        \n",
        "        # Final classifier\n",
        "        self.fc = nn.Linear(16, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Block 1\n",
        "        x = self.dropout1(F.relu(self.bn1(self.conv1(x))))\n",
        "        \n",
        "        # Block 2\n",
        "        x = self.dropout2(F.relu(self.bn2(self.conv2(x))))\n",
        "        \n",
        "        # Transition 1\n",
        "        x = F.relu(self.bn_trans1(self.transition1(x)))\n",
        "        x = self.pool1(x)\n",
        "        \n",
        "        # Block 3\n",
        "        x = self.dropout3(F.relu(self.bn3(self.conv3(x))))\n",
        "        \n",
        "        # Block 4\n",
        "        x = self.dropout4(F.relu(self.bn4(self.conv4(x))))\n",
        "        \n",
        "        # Transition 2\n",
        "        x = F.relu(self.bn_trans2(self.transition2(x)))\n",
        "        x = self.pool2(x)\n",
        "        \n",
        "        # Block 5\n",
        "        x = self.dropout5(F.relu(self.bn5(self.conv5(x))))\n",
        "        \n",
        "        # Block 6\n",
        "        x = self.dropout6(F.relu(self.bn6(self.conv6(x))))\n",
        "        \n",
        "        # Global Average Pooling\n",
        "        x = self.gap(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # Final classification\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "# Model 5: Model2 Architecture with SGD Optimization\n",
        "class OptimizedNetV5(nn.Module):\n",
        "    \"\"\"\n",
        "    Model2 architecture optimized for SGD training\n",
        "    - Same architecture as Model2 (10â†’16â†’28â†’16)\n",
        "    - Minimal dropout (0.02) for SGD compatibility\n",
        "    - Optimized for SGD with momentum\n",
        "    \"\"\"\n",
        "    def __init__(self, dropout_rate=0.02):\n",
        "        super(OptimizedNetV5, self).__init__()\n",
        "        \n",
        "        # Block 1: Better initial feature extraction (reduced channels)\n",
        "        self.conv1 = nn.Conv2d(1, 10, 3, padding=1)  # 28x28 -> 28x28\n",
        "        self.bn1 = nn.BatchNorm2d(10)\n",
        "        self.dropout1 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Block 2: Feature expansion with controlled capacity\n",
        "        self.conv2 = nn.Conv2d(10, 16, 3, padding=1)  # 28x28 -> 28x28\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.dropout2 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Transition layer: 1x1 conv for dimensionality reduction\n",
        "        self.transition1 = nn.Conv2d(16, 10, 1)  # 28x28 -> 28x28\n",
        "        self.bn_trans1 = nn.BatchNorm2d(10)\n",
        "        \n",
        "        # MaxPool after transition\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)  # 28x28 -> 14x14\n",
        "        \n",
        "        # Block 3: Mid-level features with controlled capacity\n",
        "        self.conv3 = nn.Conv2d(10, 20, 3, padding=1)  # 14x14 -> 14x14\n",
        "        self.bn3 = nn.BatchNorm2d(20)\n",
        "        self.dropout3 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Block 4: Feature expansion\n",
        "        self.conv4 = nn.Conv2d(20, 28, 3, padding=1)  # 14x14 -> 14x14\n",
        "        self.bn4 = nn.BatchNorm2d(28)\n",
        "        self.dropout4 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Transition layer: 1x1 conv for dimensionality reduction\n",
        "        self.transition2 = nn.Conv2d(28, 16, 1)  # 14x14 -> 14x14\n",
        "        self.bn_trans2 = nn.BatchNorm2d(16)\n",
        "        \n",
        "        # MaxPool after transition\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)  # 14x14 -> 7x7\n",
        "        \n",
        "        # Block 5: High-level features\n",
        "        self.conv5 = nn.Conv2d(16, 24, 3, padding=1)  # 7x7 -> 7x7\n",
        "        self.bn5 = nn.BatchNorm2d(24)\n",
        "        self.dropout5 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Block 6: Final feature extraction\n",
        "        self.conv6 = nn.Conv2d(24, 16, 3, padding=1)  # 7x7 -> 7x7\n",
        "        self.bn6 = nn.BatchNorm2d(16)\n",
        "        self.dropout6 = nn.Dropout2d(dropout_rate)\n",
        "        \n",
        "        # Global Average Pooling\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)  # 7x7 -> 1x1\n",
        "        \n",
        "        # Final classifier\n",
        "        self.fc = nn.Linear(16, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Block 1\n",
        "        x = self.dropout1(F.relu(self.bn1(self.conv1(x))))\n",
        "        \n",
        "        # Block 2\n",
        "        x = self.dropout2(F.relu(self.bn2(self.conv2(x))))\n",
        "        \n",
        "        # Transition 1\n",
        "        x = F.relu(self.bn_trans1(self.transition1(x)))\n",
        "        x = self.pool1(x)\n",
        "        \n",
        "        # Block 3\n",
        "        x = self.dropout3(F.relu(self.bn3(self.conv3(x))))\n",
        "        \n",
        "        # Block 4\n",
        "        x = self.dropout4(F.relu(self.bn4(self.conv4(x))))\n",
        "        \n",
        "        # Transition 2\n",
        "        x = F.relu(self.bn_trans2(self.transition2(x)))\n",
        "        x = self.pool2(x)\n",
        "        \n",
        "        # Block 5\n",
        "        x = self.dropout5(F.relu(self.bn5(self.conv5(x))))\n",
        "        \n",
        "        # Block 6\n",
        "        x = self.dropout6(F.relu(self.bn6(self.conv6(x))))\n",
        "        \n",
        "        # Global Average Pooling\n",
        "        x = self.gap(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # Final classification\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Selection and Configuration\n",
        "MODEL_NAME = \"Model1\"  # Options: \"Model1\", \"Model2\", \"Model3\", \"Model4\", \"Model5\"\n",
        "\n",
        "def get_model(model_name):\n",
        "    \"\"\"Get model based on name\"\"\"\n",
        "    if model_name == \"Model1\":\n",
        "        return OptimizedNet(dropout_rate=0.1)\n",
        "    elif model_name == \"Model2\":\n",
        "        return OptimizedNetV2(dropout_rate=0.05)\n",
        "    elif model_name == \"Model3\":\n",
        "        return OptimizedNetV3(dropout_rate=0.02)\n",
        "    elif model_name == \"Model4\":\n",
        "        return OptimizedNetV4(dropout_rate=0.02)\n",
        "    elif model_name == \"Model5\":\n",
        "        return OptimizedNetV5(dropout_rate=0.02)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model: {model_name}\")\n",
        "\n",
        "# Create and test the selected model\n",
        "model = get_model(MODEL_NAME).to(device)\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(f\"SELECTED MODEL: {MODEL_NAME}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if MODEL_NAME == \"Model1\":\n",
        "    print(\"Baseline Model - Original OptimizedNet (WORKING)\")\n",
        "    print(\"- Dropout: 0.1\")\n",
        "    print(\"- Channels: 8â†’16â†’32â†’16\")\n",
        "    print(\"- Status: Achieved 99.40% in 16 epochs\")\n",
        "    print(\"- Optimizer: Adam\")\n",
        "elif MODEL_NAME == \"Model2\":\n",
        "    print(\"Improved Architecture - OptimizedNetV2 (WINNER)\")\n",
        "    print(\"- Dropout: 0.05 (reduced for better learning)\")\n",
        "    print(\"- Channels: 10â†’16â†’28â†’16 (controlled capacity)\")\n",
        "    print(\"- Better initial learning dynamics\")\n",
        "    print(\"- Optimizer: Adam\")\n",
        "elif MODEL_NAME == \"Model3\":\n",
        "    print(\"Balanced Architecture - OptimizedNetV3 (SGD Optimized)\")\n",
        "    print(\"- Dropout: 0.02 (minimal for SGD)\")\n",
        "    print(\"- Channels: 10â†’16â†’32â†’16 (balanced)\")\n",
        "    print(\"- Optimized for SGD training\")\n",
        "    print(\"- Optimizer: SGD\")\n",
        "elif MODEL_NAME == \"Model4\":\n",
        "    print(\"Model1 Architecture with SGD Optimization\")\n",
        "    print(\"- Dropout: 0.02 (minimal for SGD)\")\n",
        "    print(\"- Channels: 8â†’16â†’32â†’16 (same as Model1)\")\n",
        "    print(\"- Optimizer: SGD\")\n",
        "    print(\"- Tests SGD with minimal architecture\")\n",
        "elif MODEL_NAME == \"Model5\":\n",
        "    print(\"Model2 Architecture with SGD Optimization (RECOMMENDED)\")\n",
        "    print(\"- Dropout: 0.02 (minimal for SGD)\")\n",
        "    print(\"- Channels: 10â†’16â†’28â†’16 (same as Model2)\")\n",
        "    print(\"- Optimizer: SGD\")\n",
        "    print(\"- Tests SGD with optimal architecture - POTENTIAL ULTIMATE WINNER\")\n",
        "\n",
        "summary(model, input_size=(1, 28, 28))\n",
        "\n",
        "# Count total parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nTotal Parameters: {total_params:,}\")\n",
        "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
        "print(f\"Parameter Constraint: {'âœ“ PASS' if total_params < 20000 else 'âœ— FAIL'} (< 20,000)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Loading and Augmentation Analysis\n",
        "print(\"=\"*60)\n",
        "print(\"DATA LOADING AND AUGMENTATION ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Data loading parameters\n",
        "batch_size = 128\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "# Data augmentation and normalization\n",
        "# Note: We're NOT using data augmentation in this project\n",
        "# Only basic normalization is applied\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST normalization\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Optional: Data augmentation (commented out)\n",
        "# train_transform_augmented = transforms.Compose([\n",
        "#     transforms.RandomRotation(degrees=5),\n",
        "#     transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize((0.1307,), (0.3081,))\n",
        "# ])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True, transform=train_transform),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=test_transform),\n",
        "    batch_size=batch_size, shuffle=False, **kwargs)\n",
        "\n",
        "print(\"Data Augmentation Status:\")\n",
        "print(\"âŒ NO DATA AUGMENTATION USED\")\n",
        "print(\"âœ… Only basic normalization applied\")\n",
        "print(f\"\\nBatch Size: {batch_size}\")\n",
        "print(f\"Train Samples: {len(train_loader.dataset):,}\")\n",
        "print(f\"Test Samples: {len(test_loader.dataset):,}\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training and Testing Functions\n",
        "def train(model, device, train_loader, optimizer, epoch, scheduler=None):\n",
        "    \"\"\"Training function with progress tracking\"\"\"\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    processed = 0\n",
        "    \n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        processed += len(data)\n",
        "        \n",
        "        pbar.set_description(desc=f'Epoch {epoch}: Loss={loss.item():.4f}, Acc={100.*correct/processed:.2f}%, LR={optimizer.param_groups[0][\"lr\"]:.6f}')\n",
        "    \n",
        "    avg_loss = train_loss / len(train_loader)\n",
        "    accuracy = 100. * correct / processed\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    \"\"\"Testing function\"\"\"\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    \n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    return test_loss, accuracy\n",
        "\n",
        "def plot_training_history(train_losses, train_accs, test_losses, test_accs):\n",
        "    \"\"\"Plot training and validation curves\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    \n",
        "    # Loss plot\n",
        "    ax1.plot(train_losses, label='Train Loss', color='blue')\n",
        "    ax1.plot(test_losses, label='Test Loss', color='red')\n",
        "    ax1.set_title('Training and Test Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "    \n",
        "    # Accuracy plot\n",
        "    ax2.plot(train_accs, label='Train Accuracy', color='blue')\n",
        "    ax2.plot(test_accs, label='Test Accuracy', color='red')\n",
        "    ax2.axhline(y=99.4, color='green', linestyle='--', label='Target (99.4%)')\n",
        "    ax2.set_title('Training and Test Accuracy')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy (%)')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"Training and testing functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Loop with Optimizer Configuration\n",
        "print(\"=\"*60)\n",
        "print(\"TRAINING CONFIGURATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Optimizer configuration based on model\n",
        "if MODEL_NAME in [\"Model1\", \"Model2\"]:\n",
        "    # Adam optimizers for original models\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "    max_lr = 0.01\n",
        "    pct_start = 0.3\n",
        "    opt_name = \"Adam\"\n",
        "elif MODEL_NAME in [\"Model3\", \"Model4\", \"Model5\"]:\n",
        "    # SGD optimizers for SGD-optimized models\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
        "    max_lr = 0.05\n",
        "    pct_start = 0.3\n",
        "    opt_name = \"SGD\"\n",
        "else:\n",
        "    raise ValueError(f\"Unknown model: {MODEL_NAME}\")\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer, \n",
        "    max_lr=max_lr, \n",
        "    epochs=20, \n",
        "    steps_per_epoch=len(train_loader),\n",
        "    pct_start=pct_start,\n",
        "    anneal_strategy='cos'\n",
        ")\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 20\n",
        "target_accuracy = 99.4\n",
        "\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Optimizer: {opt_name} (lr={optimizer.param_groups[0]['lr']}, weight_decay={optimizer.param_groups[0]['weight_decay']})\")\n",
        "print(f\"Scheduler: OneCycleLR (max_lr={max_lr}, pct_start={pct_start})\")\n",
        "print(f\"Epochs: {num_epochs}\")\n",
        "print(f\"Target Accuracy: {target_accuracy}%\")\n",
        "print(f\"Parameter Count: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Training history\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "# Training loop\n",
        "start_time = time.time()\n",
        "best_accuracy = 0\n",
        "best_epoch = 0\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Training\n",
        "    train_loss, train_acc = train(model, device, train_loader, optimizer, epoch, scheduler)\n",
        "    \n",
        "    # Testing\n",
        "    test_loss, test_acc = test(model, device, test_loader)\n",
        "    \n",
        "    # Store results\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_acc)\n",
        "    \n",
        "    # Check for best accuracy\n",
        "    if test_acc > best_accuracy:\n",
        "        best_accuracy = test_acc\n",
        "        best_epoch = epoch\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "    \n",
        "    # Print epoch summary\n",
        "    status = \"âœ“ ACHIEVED\" if test_acc >= target_accuracy else \"âœ— NOT ACHIEVED\"\n",
        "    print(f\"Epoch {epoch} Summary:\")\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"Valid Loss: {test_loss:.4f} | Valid Acc: {test_acc:.2f}%\")\n",
        "    print(f\"Target: {target_accuracy}% | Status: {status}\")\n",
        "    \n",
        "    if test_acc >= target_accuracy:\n",
        "        print(f\"ðŸŽ¯ New Best Validation Accuracy: {test_acc:.2f}%\")\n",
        "        print(f\"ðŸŽ‰ TARGET ACHIEVED! Validation Accuracy: {test_acc:.2f}% >= {target_accuracy}%\")\n",
        "        print(\"Target reached, but continuing training for full analysis...\")\n",
        "    \n",
        "    print(f\"Best Valid Acc so far: {best_accuracy:.2f}% (epoch {best_epoch})\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Training completed\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
        "print(f\"Final best accuracy: {best_accuracy:.2f}% at epoch {best_epoch}\")\n",
        "\n",
        "# Plot training history\n",
        "plot_training_history(train_losses, train_accuracies, test_losses, test_accuracies)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Analysis and Model Comparison\n",
        "print(\"=\"*60)\n",
        "print(\"FINAL MODEL ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load best model\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "# Final evaluation on test set only\n",
        "final_test_loss, final_test_acc = test(model, device, test_loader)\n",
        "\n",
        "# Get final training metrics from the last epoch (not by running test on train set)\n",
        "final_train_loss = train_losses[-1]  # Last epoch training loss\n",
        "final_train_acc = train_accuracies[-1]  # Last epoch training accuracy\n",
        "\n",
        "print(\"Final evaluation results:\")\n",
        "print(f\"Final Training Metrics: Train Loss: {final_train_loss:.4f} | Train Acc: {final_train_acc:.2f}%\")\n",
        "print(f\"Final Validation Metrics: Valid Loss: {final_test_loss:.4f} | Valid Acc: {final_test_acc:.2f}%\")\n",
        "\n",
        "# Train-Val Gap Analysis\n",
        "train_val_gap = final_train_acc - final_test_acc\n",
        "gap_status = \"âœ“ GOOD\" if abs(train_val_gap) <= 0.3 else \"âœ— HIGH\"\n",
        "print(f\"\\nTrain-Val Gap Analysis:\")\n",
        "print(f\"Train-Val Gap: {train_val_gap:+.2f}%\")\n",
        "print(f\"Gap Status: {gap_status}\")\n",
        "\n",
        "# Architecture Summary\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"\\nArchitecture Summary:\")\n",
        "print(f\"â€¢ Total Parameters: {total_params:,}\")\n",
        "print(f\"â€¢ Trainable Parameters: {total_params:,}\")\n",
        "print(f\"â€¢ Parameter Constraint: {'âœ“ PASS' if total_params < 20000 else 'âœ— FAIL'} (< 20,000)\")\n",
        "print(f\"â€¢ Total Epochs Trained: {num_epochs}\")\n",
        "print(f\"â€¢ Epoch Constraint: {'âœ“ PASS' if num_epochs <= 20 else 'âœ— FAIL'} (â‰¤ 20)\")\n",
        "print(f\"â€¢ Best Validation Accuracy: {best_accuracy:.2f}% (epoch {best_epoch})\")\n",
        "print(f\"â€¢ Final Validation Accuracy: {final_test_acc:.2f}%\")\n",
        "print(f\"â€¢ Target Achievement: {'âœ“ ACHIEVED' if final_test_acc >= target_accuracy else 'âœ— NOT ACHIEVED'} (â‰¥ {target_accuracy}%)\")\n",
        "\n",
        "# Key Architectural Features\n",
        "print(f\"\\nKey Architectural Features:\")\n",
        "print(f\"âœ“ Batch Normalization: After each convolution layer\")\n",
        "print(f\"âœ“ Dropout: Strategic placement with rate {model.dropout1.p if hasattr(model, 'dropout1') else 'varies'}\")\n",
        "print(f\"âœ“ Global Average Pooling: Replaces fully connected layers\")\n",
        "print(f\"âœ“ 1x1 Convolutions: Used in transition layers\")\n",
        "print(f\"âœ“ Proper Layer Ordering: Conv -> BN -> ReLU -> Dropout\")\n",
        "print(f\"âœ“ Transition Layers: Dimensionality reduction before pooling\")\n",
        "\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Comparison and Quick Testing\n",
        "print(\"=\"*60)\n",
        "print(\"MODEL COMPARISON AND QUICK TESTING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Test all models quickly to compare parameters\n",
        "models_info = {}\n",
        "for model_name in [\"Model1\", \"Model2\", \"Model3\", \"Model4\", \"Model5\"]:\n",
        "    test_model = get_model(model_name)\n",
        "    params = sum(p.numel() for p in test_model.parameters())\n",
        "    models_info[model_name] = {\n",
        "        'params': params,\n",
        "        'under_20k': params < 20000\n",
        "    }\n",
        "\n",
        "print(\"Parameter Comparison:\")\n",
        "for model_name, info in models_info.items():\n",
        "    status = \"âœ“ PASS\" if info['under_20k'] else \"âœ— FAIL\"\n",
        "    print(f\"  {model_name}: {info['params']:,} parameters ({status})\")\n",
        "\n",
        "print(f\"\\nModel Descriptions:\")\n",
        "print(f\"  Model1: Baseline (17,442 params) - Adam - WORKING\")\n",
        "print(f\"  Model2: Improved Architecture (16,480 params) - Adam - WINNER\")\n",
        "print(f\"  Model3: SGD Optimized (18,440 params) - SGD\")\n",
        "print(f\"  Model4: Model1 + SGD (17,442 params) - SGD - Tests SGD with minimal arch\")\n",
        "print(f\"  Model5: Model2 + SGD (16,480 params) - SGD - POTENTIAL ULTIMATE WINNER\")\n",
        "\n",
        "print(f\"\\nTo test a different model, change MODEL_NAME in cell 1:\")\n",
        "print(f\"  MODEL_NAME = 'Model1'  # Baseline with Adam\")\n",
        "print(f\"  MODEL_NAME = 'Model2'  # Current winner with Adam\")\n",
        "print(f\"  MODEL_NAME = 'Model3'  # SGD with balanced architecture\")\n",
        "print(f\"  MODEL_NAME = 'Model4'  # SGD with minimal architecture\")\n",
        "print(f\"  MODEL_NAME = 'Model5'  # SGD with optimal architecture - RECOMMENDED\")\n",
        "\n",
        "print(f\"\\nCurrent Model: {MODEL_NAME}\")\n",
        "print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
